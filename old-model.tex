
We adapt the definition of Ramirez and Geffner \shortcite{ramirez2010probabilistic} used for offline goal recognition to the problem of  online plan intervention.
\subsection{Preliminaries: STRIPS and Offline Goal Recognition}
Following STRIPS \cite{fikes1971strips}, we define a planning problem as a tuple $ P = \langle F, A, I, G \rangle$ where $F$ is the set of fluents, $I\subseteq F$ is the initial state, $G  \subseteq F$ represents the set of goal states and $A$ is the set of actions. Each action $a \in A$ is a triple $a=\langle Pre(a), Add(a), Del(a)\rangle$ that consists of preconditions, add and delete effects respectively, where $Pre(a), Add(a), Del(a)$ are all subsets of $F$. An action $a$ is applicable in a state $s$ if preconditions of $a$ are true in $s$; $pre(a) \in s$. If an action $a$ is executed in state $s$ (noted as $\phi(s,a)$), it results in a new state $s^{\prime} = (s \setminus del(a) \cup add(a))$.  The solution to $P$ is a plan $\pi = \{a_1, \dots ,a_k\}$ of length $k$ that modifies $I$ into $G$ by execution of actions $a_1, \dots ,a_k$.  The domains are discrete with unit-cost actions.

Ramirez and Geffner \shortcite{ramirez2010probabilistic} define an offline goal recognition problem as $T= \langle D, \mathcal{G}, O, Pr \rangle$ where $D=\langle F, A, I \rangle$ is a planning domain, $\mathcal{G}$ is the possible set of goals, and $\mathcal{G} \subseteq F$. $Pr$ is a prior probability distribution over the set of goals $\mathcal{G}$. An observation sequence $O = o_1, \ldots , o_m$ are actions $o_i \in A, i \in[1,m]$. A solution to the goal recognition problem is a probability distribution over $G \in \mathcal{G}$ indicating the relative likelihood of the goals in $\mathcal{G}$.

\subsection{The Online Intervention Problem ($\mathcal{I}$)}
Plan intervention specially focuses on \textit{timely recognition} (i.e., the time the observer recognizes  that the user will reach $G_u$). 
Thus, it bears some similarity to the goal recognition problem except we want the agent to consider all potential sources of damage regardless of initial likelihood, which leads to two differences: (1) we partition $\mathcal{G} = \lbrace G_d \cup G_u \rbrace$ into desirable and undesirable, and (2)  goal probabilities, $Pr$ are less reasonable in the intervention setting because desirable goals have a naturally high likelihood while undesirable goals have a naturally low likelihood.  
%% The plan intervention problem  adapts the goal recognition problem by removing the probabilities \emph{say why??} and partitions the set of goals into desirable and undesirable.
Although we omit $Pr$ in this initial treatment, they can be addressed in future work.

In an intervention scenario, the user agent ($\mu$) solves the planning problem $P_{\mu}=\langle F_{\mu},A_{\mu},I_{\mu},G_{d} \rangle$, where $F_{\mu} \subset F$, $A_{\mu} \subset A$, $I_{\mu} \subset I$ and $G_{d} \subseteq F_{\mu}$. The attacker agent ($\alpha$), if present,  solves the planning problem $P_{\alpha}=\langle F_{\alpha},A_{\alpha},I_{\alpha},G_{u} \rangle$, where $F_{\alpha} \subset F$, $A_{\alpha} \subset A$, $I_{\alpha} \subset I$ and $G_{d} \subseteq F_{\mu}$. In a hidden attack opportunity  $F_{\mu} \subset F_{\alpha}$. When an active opportunistic adversary is present  $F_{\mu} \subset F_{\alpha}$ and $I_{\mu} \subset I_{\alpha}$ and  $A_{\mu} \subset A_{\alpha}$. We define the state in the common environment ($s_e \subseteq F_e$) shared by the observer, $\mu$ and $\alpha$, where $F_e \subseteq \lbrace F_{\mu} \cup F_{\alpha}\rbrace$. Only a subset of propositions in $F_e$ are observable and modifiable by both $\alpha$ and $\mu$. Execution of actions in the plans $\pi_{\mu}$ and $\pi_{\alpha}$ by $\mu$ and $\alpha$ on $s_e$ is based on $A_{\mu}$ and $A_{\alpha}$ respectively. For this paper, when an active opportunistic attacker is present, we define \textit{opportunistic execution of actions} as: for two actions $a_{\mu} \in \pi_{\mu}$ and $a_{\alpha} \in \pi_{\alpha}$ both applicable in $s_e$, $a_{\alpha}$ executes first and modifies $s_e$ to $s_{e\prime}$. Then $a_{\mu}$ executes in $s_{e\prime}$ if $a_{\mu}$ is applicable in $s_{e\prime}$. In other words, as in real world scenarios, our attacker will exploit every available opportunity to mislead the user toward $G_u$. Some opportunistic actions could simply be ignored as nuisances because they are not oriented toward $G_u$, while others will definitely lead the user toward $G_u$. In the hidden attack opportunity, only $a_{\mu} \in \pi_{\mu}$ is executed when $a_{\mu}$ is applicable in $s_e$, which modifies to $s_{e\prime}$. For this case, \textit{opportunistic execution of actions} is undefined. 

Similar to the active attacker case, some actions the user will execute may be common harmless actions while, a executing other actions will definitely trigger $G_u$. The intervention process need to identify the actions that will eventually trigger $G_u$, called \textit{critical actions} and be able to filter out nuisance actions and common harmless actions. In the case of a opportunistic adversary, the observer incrementally observes the sequence of actions $O \in \lbrace\pi_\alpha\cup \pi_\mu\rbrace$, whereas, in the hidden attack opportunity $O \in \lbrace\pi_\mu\rbrace$.

Executing a sequence of actions $\pi$ ($ \left|\pi\right|=n$) in state $s$ given as:\\
$\Phi (s,\pi)=\begin{cases}
                 \ \Phi(\phi(s,a_1),(a_2,\ldots,a_n)),& \text{if } \pi \neq \emptyset\\
                          s,& \text{otherwise}
                        \end{cases}$
\\Using the definition for $\Phi (s,\pi)$, we define a \textbf{critical action} $\kappa $ as $a_i \in \pi$ and $i=1,\ldots,n$ such that $\Phi(s,\lbrace a_i\ldots a_n\rbrace) \models G_u$. As $i$ gets closer to $n$, the attack becomes imminent. $i=n$ indicates that the undesirable state will occur when the last action in the sequence is executed, which is also the maximum possible wait time for the observer to intervene. We refer to this type of intervention as just-in-time intervention, which is cybersecurity situations (e.g., stopping user just before clicking on a phishing link). When $i<n$, it indicates that an attack is developing but not imminent (i.e., early-warning intervention) as shown in the example in Figure \ref{fig:multi} (bottom) or path C in Figure \ref{fig:single}). In this paper, we only recognize critical actions for $i=n$. In either case, from the perspective of the user, $G_d$ has not been achieved, but the user needs to be warned when $\kappa$ is observed to avoid $G_u$ ensure safety. Intervention in long running attacks will be studied in future work.
%\debug{mak}{use a definition here?}

\textbf{Plan intervention problem} $\mathcal{I} = \langle D, O, G_u, G_d \rangle$ consists of a planning domain $D$ and a sequence of observed actions  $O$, a set of undesirable states $G_u \subseteq F$, a set of desirable states $G_d \subseteq F$ ($G_u \neq G_d$). A solution to $\mathcal{I}$ is a sequence of decisions corresponding to actions in $O$ indicating whether each action was identified as requiring intervention. 

Intervention decisions are made in an online setting when observations appear incrementally and include actions executed by the attacker or the user.  We make several assumptions about the three actors. \emph{(Observability)} The observer has full observability, while the user can observe everything except $G_u$ and, in problems with an attacker,  it can  observe everything \emph{except} the actions of the attacker and state resulting from those actions.
The observer know about $G_d$ and $G_u$ and helps avoid $G_u$.
\emph{(Plans)}
The user follows a satisfycing plan to reach $G_d$, but may reach the hidden $G_u$ unwittingly.
The attacker follows a satisfycing plan to reach $G_u$ and we we assume the plan to reach $G_u$ has a common prefix with a plan to reach $G_d$. 
Throughout execution, the user and the attacker stick to their original plans.
Also, they prefer the shortest plan to achieving their objective.
\debug{sach}{isn't this saying the agents are optimal? They are not} 
\emph{(Attacker)}
Finally, the attacker cannot undo a user's actions; this restriction follows from many security domains where an attacker is a remote entity that only responds to user or system actions.

