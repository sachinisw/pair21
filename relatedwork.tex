\section{Related Work}
\label{sec:relatedwork}
Closely related areas of literature for this work is plan and goal recognition. Plan recognition is the problem of inferring the course of action (i.e., plan) an actor may take towards achieving a goal from a sequence of observations \cite{schmidt1978plan,kautz1986generalized}. The constructed plan, if followed to completion, is expected to result in states that correspond to goals of the actor, which in turn presupposes that the actor intends to achieve those goals. 
Plan/goal recognition approaches in the literature explore both domain-dependent and independent methods. In domain-dependent methods, agents rely heavliy on domain knowledge for inference. For example, Kabanza et al. \shortcite{kabanza2010rts}  presents a solution that recognizes an agent’s adversarial intent by mapping observations made to date to a plan library. Boddy et al. \shortcite{boddy2005course} discuss how to construct and manipulate domain models that describe behaviors of adversaries in computer
security domain and use these models to generate plans. Another approach uses
Goal Driven Autonomy (GDA) that allows agents to continuously monitor the current plan’s execution and assess if the current state matches with expectation (Klenk et al.2013)\nocite{aha2013gda}. 

Other work attempts to separate knowledge dependency by allowing the agent to learn from observations (Jaidee et al. 2001)\nocite{jaidee2011gda}. In contrast, domain-independent goal recognition uses planning to infer agent’s goals. Ramirez and Geffner \shortcite{ramirez2009plan,ramirez2010probabilistic} used an existing planner to generate hypotheses from observations to infer a single agent's plan. Their approaches offer advantages of being more adaptive to input as well as exploiting existing planning systems and plan representations. Their first approach computed the set of goals that can be achieved by optimal plans that match the observations. The second approach removed the optimality constraint and computed a probability distribution across likely goals. \cite{ramirez2010probabilistic}. Keren et al. \shortcite{keren2014grd} introduced the worst-case distinctiveness (wcd) metric as a measurement of the ease of performing goal recognition in a domain. The wcd problem finds the longest sequence of actions an agent can execute while hiding its goal. They show that by limiting the set of available actions in the model wcd can be minimized, which will allow the agent to reveal it's goal as early as possible.

Landmarks are used in recognition to minimize the number of hypotheses the agent has to evaluate, thus improving the effeciency. Pozanco et al. \shortcite{pozanco2018counterplanning} combines probabilistic plan recognition and landmarks to counterplan and block an opponent's goal achievement. This is approach uses offline recognition where all observations are made available apriori. In plan recognition, identifying the plan at the right time is not a priority. In intervention this is critical because we want to reduce false alarms and misses. Our approach complements (and improves) existing recognition methods by using classifiers to recognize intervention and identifying salient features that may be useful in generating explanations for intervention.

%For explnations, causal links have been used in the literature to reason about intention \cite{hong2001goal,farrell2017}. When there is disparity in user's understanding of the domain, model reconciliation attempts to address the knowledge gap with explanations \cite{chakraborti2019}.

%Wilkins et al.\shortcite{wilkins2003} underscored the importance of designing automated  support for humans monitoring activities of agent teams in data-rich, dynamic environments. They introduced the concept of ``\textit{value of alert} (VOA)'', to decide when to alert a human user to valuable information about the domain itself or active adversaries. VOA uses domain specifc knowledge to heuristically estimate value of information (VOI). The alerts need to be timely and not overwhelm the human user. We present \textit{online plan intervention} as an alternative solution to deciding when to alert.


