\section{Introduction}\label{sec:intro}
When an agent is executing a plan to achieve some goal, it’s progress may be challenged by unforeseen changes such as an unexpected modification to the environment or an adversary subverting the agent’s
goal. In these situations, a passive observer intervening to help the agent reach it’s intended goal will be beneficial. Intervention is
different from the typical plan recognition problem because we assume the observed agent pursues desirable goals while avoiding undesirable states. The observer must (1) monitor actions/state unobtrusively to predict trajectories of the observed agent and (2) assist the observed agent to safely complete the task with timely interrupts.

We model online plan intervention in a competitive environment where three agents operate: (1) a human user, (2) an attacker (human or a software agent) and (3) an observer (a software agent) who will interrupt the user. The source of competition in the domain is either the attacker who leverages user's progress to achieve it's own goal or the user's uncertainty (e.g., hidden information, problem complexity). The user in the domain is pursuing a desirable goal while avoiding undesirable states. The observer, akin to the automated support system, acts as a cognitive assistant to the user.

Intervention needs to occur in different times during plan execution, based on which the user can plan to correct course or rely on the observer to make suggestions (e.g., counterplanning, alert and block action). In this work we focus on identifying critical actions, which if unchecked will definitely trigger the undesirable state. Intervention is useful in both online settings, where undesirable states may arrive incrementally (e.g., learning strategies in games) and in offline settings where observations are available prior to intervention (e.g.,troubleshooting execution).

We train a classifier with domain-independant features the observer uses to decide whether to intervene. We propose two methods: \textit{The Full method} uses a variation of the relaxed plan graph \cite{blum1997fast} to model the desirable, undesirable, and neutral states that are reachable at different depths. From the graph, we extract domain independent features: risk, desirability, distances remaining to desirable goal and undesirable states and  active landmarks (defined in Section \ref{sec:problemstatement} percentage. \textit{The Partial method} uses plan similarity metrics as features to identify risky actions without enumerating the full plan space. The classifier is then used to predict intervention on previously unseen observations.


The contributions of this paper include: (1) formalizing the online intervention problem, (2) introducing Full and Partial methods for estimating the criticality of the current state, (3) presenting an approach that learns to classify an observation as intervention or not, (4) presenting a causal model that explains intervention, and (6) introducing a new plan intervention benchmark domain called Rush Hour.
